---
title: "Event study"
format: html
---

Event studies are a common approach in econometrics to assess the impact of an event such as earnings announcements, mergers and acquisitions, regulatory changes, or macroeconomic news.

The goal is to estimate the portion of post-event change that can be attributed to the event itself, isolating it from other market and firm-specific effects. We do this by predicting what would have happened if the event had not occurred (the counterfactual) and comparing it to the actual observed data to estimate the resulting "abnormal returns" (AR).

To determine the normal return we can adopt the market model [@MacKinlay1997]
$$
D_{it} = \alpha_i + \beta_i D_{gt} + \epsilon_{it}
$$

And the AR is 
$$
AR_{it} = D_{it} – (\alpha_i + \beta_i D_{gt})
$$

Where:

$D_{it}$ is *`r credit_card.target_label()`* (differenced) $i$ at time $t$,
$\alpha_i$ is the firm-specific *`r credit_card.target_label()`* (differenced),
$\beta_i$ is the sensitivity of firm value $i$ to the group $g$,
$D_{gt}$ is the group *`r credit_card.target_label()`* (differenced) at time $t$, and
$\varepsilon_{it}$ is the error term for $i$ at time $t$.

## Evaluation

The joint test problem arises because we are effectively testing two things: the counterfactual model and the event impact. Consequently, we need an approach to accurately evaluate if our counterfactual model results are credible. One approach is to test using observations where we expect no effect; if there were resulting "abnormal returns" this would point to an issue with the model [see @HuntingtonKlein, chap 17.3.3]. 

```{r}
library(lme4)
library(caret)
library(Metrics)

source('functions.R')

run_timeseries_cv <- function(data, formula_string, model_func = lm, predict_func= predict, initial_window = 9, horizon = 1, dependent_var = "value_diff", fixedWindow = TRUE) {
    set.seed(123)  # For reproducibility
    # Create 5-fold cross-validation indices
    slices <- caret::createTimeSlices(1:nrow(data), initial_window, horizon, fixedWindow = fixedWindow)
    results <- list()

    for(i in 1:length(slices$train)) {
        # Extract training and testing data using the indices
        training_indices <- slices$train[[i]]
        validation_indices <- slices$test[[i]]
        training_set <- data |> filter(Quarter %in% data$Quarter[training_indices])
        validation_set <- data |> filter(Quarter %in% data$Quarter[validation_indices])

        # Skip if validation set contains FirmID levels not present in training set
        if(any(!(validation_set$IDRSSD %in% training_set$IDRSSD))) {
          next
        }
        # Fit model on the training data
        model <- model_func(formula_string, data = training_set)

        # Predict on the validation data
        predictions <- predict_func(model, newdata = validation_set)
        
        # Evaluate the model (you can choose your evaluation metric, e.g., RMSE)
        rmse_value <- Metrics::rmse(validation_set[[dependent_var]], predictions)

        # Store the results
        results[[i]] <- rmse_value
    }

    # Calculate the average performance across all slices
    average_performance <- mean(unlist(results))
    average_performance
}

# run_timeseries_cv_multiple_firms <- function(data, formula_string, model_func = lm, predict_func= predict, initial_window = 3, horizon = 1, dependent_var = "value_diff") {
#     # Split the data by firm
#     firm_data_list <- split(data, data$IDRSSD)

#     # Perform cross-validation for each firm and calculate mean RMSE per firm
#     cv_results <- map(firm_data_list, ~time_series_cv_per_firm(.x, formula_string, model_func, predict_func, initial_window, horizon, dependent_var)) |>
#                   map_df(~mutate(.x, MeanRMSE = mean(RMSE)), .id = "IDRSSD")

#     # Calculate the overall mean RMSE across all firms
#     overall_mean_rmse <- mean(cv_results$MeanRMSE)

#     return(list(PerFirmRMSE = cv_results, OverallMeanRMSE = overall_mean_rmse))
# }


# time_series_cv_per_firm <- function(data, formula_string, model_func=lm, predict_func = predict, initial_window = 3, horizon = 1, dependent_var = "value_diff") {
#   total_points <- nrow(data)
#   results <- tibble()

#   for (start_idx in 1:(total_points - initial_window - horizon + 1)) {
#     end_idx <- start_idx + initial_window - 1 + horizon
#     if (end_idx > total_points) {
#       break
#     }

#     training_set <- data[start_idx:(end_idx - horizon), ]
#     validation_set <- data[(end_idx - horizon + 1):end_idx, ]

#     model <- model_func(formula_string, data = training_set)
#     validation_preds <- predict_func(model, newdata = validation_set)

#     rmse_value <- Metrics::rmse(validation_set[[dependent_var]], validation_preds)

#     results <- rbind(results, tibble(Start = start_idx, RMSE = rmse_value))
#   }

#   return(results)
# }

run_kfold_validation <- function(data, formula_string,model_func = lm, predict_func= predict) {
    set.seed(123)  # For reproducibility
    folds <- createFolds(data$IDRSSD, k = 5)
    results <- data.frame()  # Data frame to store results
    formula <- as.formula(formula_string)

    for(i in 1:length(folds)) {
        # Split the data into training and validation sets
        training_set <- data[-folds[[i]], ]
        validation_set <- data[folds[[i]], ]
        
        model <- model_func(formula, data = training_set)
        # Predict on the validation set
        validation_preds <- predict_func(model, newdata = validation_set)

        # Evaluate the model (using an appropriate metric like RMSE)
        rmse_value <- Metrics::rmse(validation_set$value_diff, validation_preds)

        # Store the results
        results <- rbind(results, data.frame(Fold = i, RMSE = rmse_value))
    }

    # Calculate the average performance across all folds
    average_performance <- mean(results$RMSE)
    return(average_performance)
}

data <- get_model_data() |> drop_na(UBPRE524.diff)

estimation_data <- data |> filter(is.na(Partnership))
event_data <- data |> filter(!is.na(Partnership))

results <- tibble(model = character(),
                    rsme = numeric())

#estimation_data |> select(starts_with("UBPRE524")) |>View()

market_formula <- "UBPRE524.diff ~ UBPRE524.group.diff"
fixed_formula <- "UBPRE524.diff ~ BankType*factor(Quarter) + (1|BankName)"

fixed_2way_formula <- "UBPRE524.diff ~  UBPRE524.group.diff | BankName + Quarter"
mixed_formula <- "UBPRE524.diff ~ BankType + (1 | BankName)"
hier_formula <- "UBPRE524.diff ~ BankType + (1 | BankType/IDRSSD) + UBPRE425.diff "


# Fit the models
m_market <- estimation_data |> model( lm = TSLM(as.formula(market_formula)))

fit <- estimation_data |> 
                model( lm = TSLM(UBPRE524.diff ~ UBPRE524.group.diff + trend() + season()))

forecast(fit, newdata = event_data)

event_data |> select(UBPRE524.group.diff)

augmented_data <- fit |>
                    augment()


RMSE(augmented_data$.fitted, augmented_data$.innov)

estimation_data |>
    autoplot(UBPRE524.diff) + 
    geom_line(data= fitted(m), aes(y = .fitted, colour = .model)) + facet_wrap(~BankName)

m_fixed <- lmer(as.formula(fixed_formula), data = estimation_data)
m_fixed2 <- feols(as.formula(fixed_2way_formula), data = estimation_data)
m_mixed <- lm(as.formula(mixed_formula), data = estimation_data)
m_hier <- lmer(as.formula(hier_formula), data = estimation_data)
summary(m_fixed2)
coef(m_fixed2)

feols(lifeExp ~ log(gdpPercap) | country + year,
             data = gm)

AIC(logLik(m_fixed))
AIC(m_fixed2)
BIC(m_fixed2)


install.packages("fixest")
library(fixest)
# Predict normal returns for the event window
event_data <- partner_data |> filter(EventPeriod == TRUE)

augmented_data <- event_data |>
                left_join(bank_mean) |>
                mutate(AR_mean = value_diff - value_diff.est.mean,
                AR_group = value_diff - value_diff.group.mean,
                m_market.preds = predict(m_market, newdata = event_data),
                m_fixed.preds = predict(m_fixed, newdata = event_data),
                m_mixed.preds = predict(m_mixed, newdata = event_data),
                m_hier.preds = predict(m_hier, newdata = event_data),
                m_market.AR = value_diff - m_market.preds,
                m_fixed.AR = value_diff - m_fixed.preds,
                m_mixed.AR = value_diff - m_mixed.preds,
                m_hier.AR = value_diff - m_hier.preds) |>
                add_row(estimation_data)


augmented_data |> 
    filter(!is.na(partner_name)) |> 
        select(Period,acquired,partner_name, partnership, BankName, value_diff,  m_market.preds,m_fixed.preds,m_hier.preds, m_mixed.preds,value_diff.est.mean) |>
            pivot_longer(cols = 6:last_col()) |>
                ggplot(aes(x = Period, y = value, colour = name)) +
                    geom_line() + 
                    geom_vline(xintercept = 0, linetype=4) +
                    facet_wrap(~partner_name+partnership+BankName, ncol=2) + scale_x_continuous(breaks = -10:3) +scale_colour_colorblind7() 
                    

augmented_data |> filter(!is.na(partner_name), Period >=0) |> 
select(Period,partner_name, partnership, BankName, AR_mean, AR_group,m_hier.AR,m_fixed.AR, m_mixed.AR, m_market.AR) |>
pivot_longer(cols = contains("AR", ignore.case=FALSE)) |>
ggplot(aes(x = Period, y = value, colour = name)) +
    geom_line() + geom_vline(xintercept = 0, linetype=4) +
                    facet_wrap(~partner_name+partnership+BankName, ncol=2) +
                    scale_colour_colorblind7() + scale_x_continuous(breaks = 0:3)

results |> 
    add_row(model="Market", rsme = run_kfold_validation(estimation_data, market_formula)) |>
    add_row(model="Fixed", rsme = run_kfold_validation(estimation_data, fixed_formula)) |>
    add_row(model="Mixed", rsme = run_kfold_validation(estimation_data, mixed_formula)) |>
    add_row(model="Hierarchy", rsme = run_kfold_validation(estimation_data, hier_formula, lmer)) 

estimation_data |> View()

results |> 
    add_row(model="Market", rsme = run_kfold_validation(estimation_data, market_formula)) |>
    add_row(model = "Market_TS", rsme = run_timeseries_cv(estimation_data, market_formula, initial_window = 20))


```

```{r}
lmer1_formula <- "value_diff  ~ (1 | IDRSSD) + value_diff.group.mean"
result <- run_timeseries_cv(estimation_data, lmer1_formula, model_func = lmer)
result_fixed <- run_timeseries_cv(estimation_data, lmer1_formula, model_func = lmer)


results |> 
    add_row(model="IMER", rsme = run_kfold_validation(estimation_data, lmer1_formula)) |>
    add_row(model = "IMER_TS", rsme = run_timeseries_cv(estimation_data, lmer1_formula, model_func = lmer)$OverallMeanRMSE)

data =estimation_data
formula_string = lmer1_formula
model_func = lmer
```
# non_partership_banks <- bank_data |> filter(is.na(name))
# partership_banks <- bank_data |> filter(!is.na(name))

# results <- tibble(model = character(),
#                     rsme = numeric())

# lmer1_formula <- "value_diff  ~ Qtr + RRSFS.Pop + RRSFS.Pop.lag1 + (1 | IDRSSD)"
# lmer2_formula <- "value_diff  ~ Qtr + RRSFS.Pop + RRSFS.Pop.lag1 + UNRATE + UNRATE.lag1 + (1 | IDRSSD)"
# lmer3_formula <- "value_diff  ~ Qtr + RRSFS.Pop + RRSFS.Pop.lag1 + UNRATE + UNRATE.lag1 + (1 | BankType/IDRSSD)"
# lmer4_formula <- "value_diff  ~ Qtr + (1 | BankType/IDRSSD) + BankType"
# lmer5_formula <- "value_diff  ~ Qtr + value_diff.group.mean + (1 | IDRSSD)"
# lmer6_formula <- "value_diff  ~ Qtr + value_diff.group.mean + (1 | BankType)"
# lmer7_formula <- "value_diff  ~ Qtr + value_diff.median + (1 | BankType)"
# lmer8_formula <- "value_diff  ~ Qtr + value_diff.group.mean + (1 | IDRSSD) + RRSFS.Pop + RRSFS.Pop.lag1"
# lmer9_formula <- "value_diff.detrend  ~ RRSFS.Pop + RRSFS.Pop.lag1 + UNRATE + UNRATE.lag1 + (1 | IDRSSD)"

# arima1_formula <- "value_diff ~ 1 + RRSFS.Pop.mean + RRSFS.Pop.lag1.mean + UNRATE.mean + UNRATE.lag1.mean"

# results |>
# add_row(model="LMER1", rsme =run_kfold_validation(non_partership_banks, lmer1_formula)) |>
# add_row(model="LMER2", rsme =run_kfold_validation(non_partership_banks, lmer2_formula)) |> 
# add_row(model="LMER3", rsme =run_kfold_validation(non_partership_banks, lmer3_formula)) |>
# add_row(model="LMER4", rsme =run_kfold_validation(non_partership_banks, lmer4_formula)) |>
# add_row(model="LMER5", rsme =run_kfold_validation(non_partership_banks, lmer5_formula)) |>
# add_row(model="LMER6", rsme =run_kfold_validation(non_partership_banks, lmer6_formula)) |>
# add_row(model="LMER7", rsme =run_kfold_validation(non_partership_banks, lmer7_formula)) |>
# add_row(model="LMER7", rsme =run_kfold_validation(non_partership_banks, lmer8_formula))|>
# add_row(model="LMER7", rsme =run_kfold_validation(non_partership_banks, lmer9_formula))
# add_row(model="ARIMA7", rsme =run_kfold_validation_arima(non_partership_banks, arima1_formula))


# best_model <- lmer(as.formula(lmer6_formula), data = non_partership_banks)
# partership_banks$preds <- predict(best_model, newdata = partership_banks)
# partership_banks$residuals <- partership_banks$value_diff - partership_banks$preds
# # If you know the actual outcomes for the test data
# test_rmse <- Metrics::rmse(partership_banks$value_diff, partership_banks$preds)

# partership_banks |> 
#     ggplot(aes(x = Quarter, y = value_diff, colour = EventPeriod)) +
#     geom_line(aes(y = preds))+ #, color = "blue") +  # Predicted fit
#     geom_point(aes(y = value_diff), color = "red", alpha = 0.5) +  # Observed data
#     facet_wrap(~ partnership+name+BankName, scales = "free_x") +  # Separate plot for each firm
#     labs(title = "Fitted Model for Each Firm", 
#         x = "Date", 
#         y = "value_diff")

# ggplot(acs(x))

# plot(test_preds, residuals, main="Residuals vs Predicted", xlab="Predicted Values", ylab="Residuals")
# abline(h=0, col="red")

# #create Q-Q plot for residuals
# qqnorm(residuals)
# #add a straight diagonal line to the plot
# qqline(residuals) 
# plot(density(partership_banks$residuals))
# plot(density(partership_banks |> filter(EventPeriod == FALSE) |> pull(residuals)))
# plot(density(partership_banks |> filter(EventPeriod == TRUE) |> pull(residuals)))




# bank_data |> View() 
# library(lme4)
# library(splines)

# quarter(yearquarter("2010 Q2"))

# m <- lmer(value_diff.detrend  ~ Qtr + (1 | BankType/IDRSSD) + BankType+ EventPeriod + (EventPeriod | IDRSSD), data = bank_data)

# m <- lmer(value_diff  ~ Qtr + value_diff.group.mean +  (EventPeriod | IDRSSD), data = bank_data)

# m <- lmer(value_diff  ~ Qtr + RRSFS.Pop+RRSFS.Pop.lag1 + UNRATE + UNRATE.lag1 + (EventPeriod | IDRSSD), data = bank_data)

# fit_consMR <- bank_data |> as_tsibble(key=c(BankName,name, partnership), index=Quarter) |>
#                 fabletools::model(tslm = TSLM(value_diff  ~ Qtr + (1 | IDRSSD) + value_diff.group.mean+ RRSFS.Pop+RRSFS.Pop.lag1 + UNRATE + UNRATE.lag1 + (EventPeriod | IDRSSD)))


# augmented_data <- fit_consMR |>filter(!is.na(name)) |>
#                      augment()

# augmented_data |>
#   ggplot(aes(x = Quarter)) +
#   geom_line(aes(y = value_diff, colour = "Data")) +
#   geom_line(aes(y = .fitted, colour = "Fitted")) +
#   facet_wrap(~ partnership+name+BankName, scales = "free_x") +
#   labs(y = NULL,
#     title = "Percent change in US consumption expenditure"
#   ) +
#   scale_colour_manual(values=c(Data="black",Fitted="#D55E00")) +
#   guides(colour = guide_legend(title = NULL)) 

# summary(m)
# bank_data$yhat <- predict(m, re.form = NULL)  # re.form = NULL to include random effects

# subset(bank_data, partner_event == 1) |>
#     ggplot(aes(x = Quarter, y = value_diff, colour = EventPeriod)) +
#     geom_line(aes(y = yhat))+ #, color = "blue") +  # Predicted fit
#     geom_point(aes(y = value_diff), color = "red", alpha = 0.5) +  # Observed data
#     facet_wrap(~ partnership+name+BankName, scales = "free_x") +  # Separate plot for each firm
#     labs(title = "Fitted Model for Each Firm", 
#         x = "Date", 
#         y = "Return")


    
        


# # Estimate And observation data
# est_data <- bank_data |> filter(between(Period, -6, -1))
# obs_data <- bank_data |> filter(between(Period, 0, 5)) 

# m_recipe <- recipe(bank_data) |>
#   update_role(everything(), new_role = "support") |> 
#   update_role(value_diff.Bank, new_role = "outcome") |>
#   update_role(value_diff.Agg, partnership, new_role = "predictor") |>
#   step_dummy(partnership, one_hot=T)

# # obs_data <- m_recipe |>
# #             prep(obs_data) |>
# #             bake(obs_data)

# m_model <- linear_reg() |> set_engine("lm")

# m_workflow <- workflow() |>
#                 add_recipe(m_recipe) |>
#                 add_model(m_model)

# m_fitted <- m_workflow |> fit(est_data)

# y_hat <- m_fitted |> predict(obs_data)

#   #metric_set(rmse, mae, rsq)(obs_data$value_diff.Bank, .pred)


# #https://mdneuzerling.com/post/machine-learning-pipelines-with-tidymodels-and-targets/
# #In the observation period, subtract the prediction from the actual return to get the “abnormal return.” 
# obs_data_ar <- bind_cols(obs_data, y_hat) |>
#     # Using mean of estimation return
#     mutate(
#         #mean = value_diff.Bank - mean(est_data$value_diff.Bank),
#         # Then comparing to peers
#         #peers = value_diff.Bank - value_diff.Agg,
#         # Then using model fit with estimation data
#         Predicted = .pred,
#         "Actual-Predicted" = value_diff.Bank - .pred)

# #put predictions and predictions together for visualisation
# actual <- bank_data |> 
#     filter(between(Period, -6, 5)) |> 
#     select(Period, partner_event = name, BankName, partnership, Actual=value_diff.Bank) 
   
# predict <- obs_data_ar |> 
#     select(Period, partner_event = name, BankName, partnership, Predicted) 
    
# actual |> left_join(predict) |>
# as_tsibble(index=Period, key=c(partner_event, BankName)) |>
# pivot_longer(cols=c(Actual,Predicted), names_to = "Values") |> 
#     autoplot(value,aes(color = Values)) + facet_grid(partnership ~ partner_event) +
#     theme(legend.position = "top")

# ```

# ##Evaluating the model
   
# The idea is this: when there’s supposed to be an effect, and we test for an effect, we can’t tease apart what part of our findings is the effect, and what part is the counterfactual model being wrong. But what if we knew there were no effect? Then, anything we estimated could only be the counterfactual model being wrong. So if we got an effect under those conditions, we’d know our model was fishy and would have to try something else.

# So, get a bunch of different time series to test, especially those unaffected by the event. In the context of stock returns, for example, don’t just include the firm with the great announcement on a certain day, also include stocks for a bunch of other firms that had no announcement.

# Then, start doing your event study over and over again. Except each time you do it, pick a random time series to do it on, and pick a random day when the “event” is supposed to have happened.21
# ```{r}
# selected_measure |>
#     as_tibble() |>
#     filter(LargeBank==1|LargeCreditCardBank==1) |>
#     left_join(partners, relationship = "many-to-many") |>
#     rename(value_diff.Bank = value_diff) |>
#     mutate(Period = Quarter - yearquarter(acquired)) 

# ```