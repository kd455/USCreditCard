---
title: "XGBoost Model"
format: html
---

Look-Ahead Bias: If data from after the test period is included in the training set, the model may inadvertently learn information from the future (known as "look-ahead bias"). This makes the model unrealistic and potentially over-optimistic in its predictive performance

```{r}
#| warning: false
source("functions.R")
library(data.table)
library(xgboost)
library(Matrix)
library(caret)
library(patchwork)
library(shapviz)
set.seed(42)

model_data <- get_model_data()
all_data <- model_data$all_data |> as_tibble()           
partner_banks <- credit_card.partnerships.long()
estimation_data <- model_data$estimation_data
observation_data <- model_data$observation_data

log_cols_w_group <- all_data |>
                    dplyr::select(contains(".log.diff")) |> 
                    dplyr::select(-c(starts_with("UBPRE524.log.diff"))) |> names() 

log_cols_wo_group <- all_data |>
                    dplyr::select(contains(".log.diff")) |> 
                    dplyr::select(-c(starts_with("UBPRE524.log.diff"), 
                                     starts_with("UBPRE524.group"), 
                                     starts_with("UBPRE524.all")))  |> names() 


setup_train_test <- function(estimation_data, predictor_cols) {
  data <- estimation_data |> as_tibble() |>
            dplyr::select(c(UBPRE524.diff, BankName, BankType, all_of(predictor_cols), Qtr))  

  s <- createDataPartition(data$UBPRE524.diff, p = 0.70, list=FALSE)
  training <- data[s,]
  test <- data[-s,]

  # Convert the data to matrix and assign output variable
  train.outcome <- training$UBPRE524.diff
  train.predictors <- sparse.model.matrix(UBPRE524.diff ~ .-1, data = training)
  test.outcome <- test$UBPRE524.diff
  test.predictors <- sparse.model.matrix(UBPRE524.diff ~ .-1, data = test)

  # Convert the matrix objects to DMatrix objects
  dtrain <- xgb.DMatrix(train.predictors, label = train.outcome)
  dtest <- xgb.DMatrix(test.predictors, label = test.outcome)

  list(train = training, test = test, dtrain = dtrain, dtest = dtest, train.predictors = train.predictors, 
       test.predictors = test.predictors, train.outcome = train.outcome, test.outcome = test.outcome)
}

model_w_group <- setup_train_test(estimation_data, log_cols_w_group)
model_without_group <- setup_train_test(estimation_data, log_cols_wo_group)

```

```{r}
#| warning: false
#| echo: false
#| eval: false
run_xgb.cv <- function(dtrain) {
  param_grid <- expand.grid(
    eta = c(0.01, 0.1, 0.3),
    max_depth = c(3, 6, 9),
    gamma = c(0,0.1,0.2),
    colsample_bytree = c(0.5, 0.8),
    min_child_weight = c(1, 5),
    subsample = c(0.5, 0.75),
    alpha = c(0, 0.1, 1, 10),
    lambda = c(0, 0.1, 1, 10)
  )

  # Placeholder for cross-validation results
  cv_results <- list()

  for(i in seq_len(nrow(param_grid))) {
    params <- list(
      booster = "gbtree",
      eta = param_grid$eta[i],
      max_depth = param_grid$max_depth[i],
      subsample = param_grid$subsample[i],
      colsample_bytree = param_grid$colsample_bytree[i],
      min_child_weight = param_grid$min_child_weight[i],
      alpha = param_grid$alpha[i],
      lambda = param_grid$lambda[i],
      objective = "reg:squarederror"
    )
    
    cv <- xgb.cv(params = params, data = dtrain, nrounds = 100, nfold = 5, early_stopping_rounds = 10)
    cv_results[[i]] <- list(params = params, cv_metrics = cv)
  }

  best_score <- Inf
  best_model <- NULL

  for(i in seq_along(cv_results)) {
    score <- min(cv_results[[i]]$cv_metrics$evaluation_log)
    if(score < best_score) {
      best_score <- score
      best_model <- cv_results[[i]]
    }
  }
  best_model$cv_metrics[4]$evaluation_log |> readr::write_csv("data/results/xgboost_cv_metrics.csv")
  best_model$params |> as.data.frame() |> readr::write_csv("data/results/xgboost_cv_params.csv")
}

run_xgb.caret.cv <- function(train.predictors, train.outcome, n_folds, n_repeats, model_name, method = "xgbTree", seeds) {
  
  if (method == "xgbTree") {
    grid <- expand.grid(eta = c(0.01, 0.1, 0.3),
                        max_depth = c(3, 6, 9),
                        gamma = c(0, 0.1),
                        colsample_bytree = c(0.5, 0.8),
                        min_child_weight = c(1, 5),
                        subsample = c(0.5, 0.75),
                        nrounds = c(100,150,200)
                      )
  } else {
    grid <- expand.grid(eta = 0.01,
                        alpha = c(0.1, 0.5, 1),
                        lambda = c(0.1, 1, 5, 10, 15, 25, 30, 50),
                        nrounds = 100
    )
  }

  set.seed(123)
  control <- trainControl(method = "repeatedcv", number = n_folds, repeats = n_repeats, seeds = seeds)

  # Train the model
  model <- train(train.predictors, train.outcome,
                method = method,
                trControl = control,
                tuneGrid = grid,
                metric = "RMSE")

  # Save the plot
  png(glue("data/results/{method}_tuning_plot_{model_name}.png"), width = 800, height = 600)
  plot(model)
  dev.off() 

  #save tuning results
  model$results  |> readr::write_csv(glue("data/results/{method}_tuning_results_{model_name}.csv"))
  model$bestTune |> readr::write_csv(glue("data/results/{method}_best_tune_{model_name}.csv"))
  xgb.save(model = model$finalModel, fname = glue("data/results/best_{method}_{model_name}.model"))
}

# Define the number of folds and repeats
n_folds <- 5
n_repeats <- 3
seeds <- lapply(1:(n_folds * n_repeats), function(x) sample.int(1000, 144))
seeds[[length(seeds) + 1]] <- sample.int(1000, 1)  # The last seed for the final model

run_xgb.caret.cv(model_w_group$train.predictors, model_w_group$train.outcome, n_folds, n_repeats, model_name= "model_w_group", method = "xgbTree", seeds)
run_xgb.caret.cv(model_without_group$train.predictors, model_without_group$train.outcome, n_folds, n_repeats, model_name="model_without_group", method = "xgbTree", seeds)

```
## Model Selection
5-Fold Cross-Validation:

The dataset is divided into 5 equal (or nearly equal) parts, or "folds".
For each fold:
The fold is temporarily held out as the test set.
The model is trained on the remaining 4 folds combined.
The trained model is then tested on the held-out fold to evaluate its performance.
This process is repeated 5 times, once for each fold, ensuring that every data point is used exactly once as part of the test set.
The 5 performance metrics (e.g., accuracy, MSE) obtained from each fold are typically averaged to get a single performance estimate for the model.
Repeating the Process 3 Times:

After completing the 5-fold cross-validation once, you repeat the entire process 2 more times, each time with a different random division of the dataset into 5 folds.
This means you will perform a total of 15 train-test splits (3 repeats Ã— 5 folds).
For each repeat, you calculate an average performance metric over its 5 folds, just like in the single round of 5-fold cross-validation.
Finally, you might average the performance metrics across all 3 repeats to get an overall performance estimate, or you might assess the variability of the model performance across repeats.

![Caption](images/xgbTree_tuning_plot.png){#fig-xgcv}

## Model Results

On estimation data

```{r}
#| warning: false
#| results: hide
#| label: tbl-xgboost-param
#| tbl-cap: Best model parameters 
#| tbl-subcap: 
#|  - With group parameters
#|  - Without group parameters
params_w_group <- read_csv("data/results/xgbTree_best_tune_model_w_group.csv",show_col_types = FALSE) |> as.list()
params_w_group |> as_tibble() |> pivot_longer(cols = everything()) |> rmarkdown::paged_table()   

params_wo_group <- read_csv("data/results/xgbTree_best_tune_model_without_group.csv",show_col_types = FALSE) |> as.list()
params_wo_group |> as_tibble() |> pivot_longer(cols = everything()) |> rmarkdown::paged_table() 
```

## Train/Test Split Validation

```{r}
#| warning: false
#| results: hide
#| label: tbl-xgboost-test
#| tbl-cap: Comparing Accuracy Metrics for Training and Test data
#| tbl-subcap:
#| - "With Group: Accuracy Metrics on Training and Test data for sub-set of banks"
#| - "With Group: Mean Accuracy Metrics for our sub-set of banks"
#| - "With Group: Mean Accuracy Metrics across all banks"
#| - "Without Group: Accuracy Metrics on Training and Test data for sub-set of banks"
#| - "Without Group: Mean Accuracy Metrics for our sub-set of banks"
#| - "Without Group: Mean Accuracy Metrics across all banks"

bank_fit_result <- function(bankname, data, train_test = "Train", target_name = "UBPRE524.diff") {
  tryCatch({
    X_filtered <- data |> filter(BankName == bankname) |> pull(predictions)
    Y_filtered <- data |> filter(BankName == bankname) |> pull(target_name)
    caret::postResample(pred = X_filtered, obs = Y_filtered) |> t() |>as_tibble() |>
      add_column(BankName = bankname,
                 .type = train_test) |> relocate(BankName,.type)
  }, error = function(e) {
    print(glue::glue("metrics error for {bankname}: {e}"))
  })
}

run_validation <- function(model, params, banks) {
  m <- xgboost(booster = "gbtree", data = model$dtrain, params= params, nrounds=params$nrounds)

  #Training fit
  train_predictions <- predict(m, model$dtrain)
  # See Fit on unseen data: Generate Predictions
  test_predictions <- predict(m, model$dtest)

  train_result <- model$train |> mutate(predictions = train_predictions)
  test_result <- model$test |> mutate(predictions = test_predictions)

  results_est <- unique(model$train$BankName) |> 
                      map(bank_fit_result,train_result, "Train") |> list_rbind()

  results_test <- unique(estimation_data$BankName) |> 
                      map(bank_fit_result,test_result, "Test") |> list_rbind()
                        

  comb <- bind_rows(results_est, results_test) |>
            dplyr::select(BankName, .type, RMSE, MAE, Rsquared)

  summary_banks <- comb |> 
                  filter(BankName %in% banks) |> 
                    arrange(BankName,.type) |> mutate(across(where(is.numeric), \(x) round(x,3)))

  summary_mean_banks <- comb |> filter(BankName %in% banks) |> 
                          group_by(.type) |> summarise(across(where(is.numeric), \(x) mean(x, na.rm = TRUE))) |>
                          mutate(across(where(is.numeric), \(x) round(x,3)))
                            
  summary_mean_all <- comb |> group_by(.type) |> 
                        summarise(across(where(is.numeric), \(x) mean(x, na.rm = TRUE))) |> 
                        mutate(across(where(is.numeric), \(x) round(x,3)))

  shap_train <- shapviz(m,X_pred = model$dtrain,X=data.matrix(model$train.predictors))
  shap_test <- shapviz(m,X_pred = model$dtest,X=data.matrix(model$test.predictors))

  list(tbl1= summary_banks, tbl2 = summary_mean_banks, tbl3 = summary_mean_all, shap_train = shap_train, shap_test = shap_test)
}


val_results <- run_validation(model_w_group, params_w_group, unique(observation_data$BankName))
val_results$tbl1 |> rmarkdown::paged_table()
val_results$tbl2 |> rmarkdown::paged_table()
val_results$tbl3 |> rmarkdown::paged_table()

wo_val_results <- run_validation(model_without_group, params_wo_group, unique(observation_data$BankName))
wo_val_results$tbl1 |> rmarkdown::paged_table()
wo_val_results$tbl2 |> rmarkdown::paged_table()
wo_val_results$tbl3 |> rmarkdown::paged_table()
```


::: {#fig-xgboost-shap .panel-tabset}

## Feature Importance
```{r}
#| eval: false
#| warning: false
#| label: fig-plot-shap-importance-xgboost
#| fig-cap: Importance 
#| fig-subcap:
#|  - "Training with group"
#|  - "Training without group"
#|  - "Test with group"
#|  - "Test without group"
#| layout-ncol: 2

shapviz::sv_importance(val_results$shap_train, kind = "beeswarm", show_numbers = TRUE)
shapviz::sv_importance(wo_val_results$shap_train, kind = "beeswarm", show_numbers = TRUE)

shapviz::sv_importance(val_results$shap_test, kind = "beeswarm", show_numbers = TRUE)
shapviz::sv_importance(wo_val_results$shap_test, kind = "beeswarm", show_numbers = TRUE)

```

## Waterfall

```{r}
#| eval: false
#| warning: false
#| label: fig-plot-shap-waterfall-xgboost
#| fig-cap: Waterfall
#| fig-subcap:
#|  - "Training with group"
#|  - "Training without group"
#|  - "Test with group"
#|  - "Test without group"
#| layout-ncol: 2
sv_waterfall(val_results$shap_train)
sv_waterfall(wo_val_results$shap_test)

sv_waterfall(val_results$shap_test)
sv_waterfall(wo_val_results$shap_test)
```

:::


```{r}
#| warning: false
#| label: fig-plot-fit-xgboost
#| fig-cap: "Fitted vs. Observed for estimation data on sub-set of firms"
#| fig-height: 10
predictor_cols <- names(model_w_group$train)
est_data_wQuarter <- estimation_data |> dplyr::select(c(all_of(predictor_cols), Quarter, UBPRE524.Value))
bank_data_pred <- est_data_wQuarter[s,]
bank_data_test <- est_data_wQuarter[-s,]
bank_data_pred$train_pred <- train_predictions
bank_data_test$test_pred <- test_predictions
bank_results <- bind_rows(bank_data_test, bank_data_pred) |> 
                mutate(.resid = UBPRE524.diff - coalesce(test_pred,train_pred)) |>
                dplyr::select(Quarter, BankName, BankType,UBPRE524.diff,train_pred,test_pred, .resid) 
                  
bank_results |> 
  filter(BankName %in% observation_data$BankName) |> dplyr::select(-.resid) |>
    pivot_longer(cols = where(is.numeric))|> 
      ggplot(aes(x=Quarter, y = value, color=name)) + geom_line() + geom_point()+
      facet_wrap(~BankType+BankName, ncol = 1) + 
      scale_colour_manual(values = c("train_pred" = "#1B9E77", 
                                     "test_pred" = "#D55E00", 
                                      UBPRE524.diff = "darkgrey")) + 
      theme(legend.position = "top", legend.title = element_blank(), legend.direction = "vertical")

```
### Residual diagnostics {#sec-residual-check-xgboost}

XGBoost, as with other machine learning models, do not make assumptions about the distribution of residuals. But it is still worth inspecting the residuals to assess whether the model is adequately capturing the underlying patterns in the data. 

::: {#fig-xgboost-checks .panel-tabset}


### CITIBANK
```{r}
#| warning: false
#| label: fig-res-citi-xgboost
#| fig-cap: "XGBoost residuals"
bank <- "CITIBANK"
plot_bank_residual(bank, bank_results)

```

### SYNCHRONY
```{r}
#| warning: false
#| label: fig-res-sync-xgboost
#| fig-cap: "Best Bank Model residuals"
bank <- "SYNCHRONY"
plot_bank_residual(bank, bank_results)
```

### BARCLAYS
```{r}
#| warning: false
#| label: fig-res-bar-xgboost
#| fig-cap: "Model residuals"
bank <- "BARCLAYS"
plot_bank_residual(bank, bank_results)
```

### AMERICAN EXPRESS
```{r}
#| warning: false
#| label: fig-res-ae-xgboost
#| fig-cap: "Model residuals"
bank <- "AMERICAN EXPRESS"
plot_bank_residual(bank, bank_results)
```

### CAPITAL ONE

```{r}
#| warning: false
#| label: fig-res-cap-xgboost
#| fig-cap: "Model residuals"
bank <-  "CAPITAL ONE"
plot_bank_residual(bank, bank_results)
```

:::

### residuals White Noise

XGBoost is not inherently designed for time series analysis, but can expose where the model has missed temporal structure or seasonality.

The sub-set of banks analysed in @sec-residual-check-xgboost did not exhibit significant autocorrelation.

@tbl-lb-xgboost lists banks where we reject the null hypothesis of the Ljungâ€“Box test, indicating the presence of significant autocorrelation in the residuals. 

Banks not listed have residuals that are indistinguishable from a white noise series i.e. have uncorrelated observations and with constant variance.
```{r}
#| warning: false
#| label: tbl-lb-xgboost
#| tbl-cap: Ljungâ€“Box results where P-value < Significance Level of 0.05
bank_results |>
 as_tsibble(index = Quarter, key = BankName) |> features(.resid, ljung_box) |> 
  mutate(across(where(is.numeric), \(x) round(x,4))) |> 
    filter(lb_pvalue <= 0.05)|> dplyr::select(BankName,lb_stat,lb_pvalue) |>
      rmarkdown::paged_table()
```

## Prediction 

```{r}
#| warning: false
#| results: hide
#|
# Convert the data to matrix and assign output variable
estimation.outcome <- estimation_data$UBPRE524.diff
estimation.predictors <- sparse.model.matrix(UBPRE524.diff ~ .-1, data = estimation_data |> as_tibble() |> dplyr::select(predictor_cols))
observation.outcome <- observation_data$UBPRE524.diff
observation.predictors <- sparse.model.matrix(UBPRE524.diff ~ .-1, data = observation_data  |> as_tibble() |> dplyr::select(predictor_cols))

# Convert the matrix objects to DMatrix objects
dEst <- xgb.DMatrix(estimation.predictors, label = estimation.outcome)
dObs <- xgb.DMatrix(observation.predictors, label = observation.outcome)

#train model   
m <- xgboost(booster = "gbtree", data = dEst, params= params, nrounds=params$nrounds)
#predit
predictions <- predict(m, dObs)
result_obs <- observation_data |> mutate(prediction = predictions)

map_prediction_to_partner <- function(Bank, Partner, obs_data_w_qtr, all_data) {
  event_data <- obs_data_w_qtr |> 
                  filter(BankName == Bank) |> 
                    filter(!!as.name(Partner) >=0) |> head(8) 

  min_qtr <- event_data |> filter(Quarter == min(Quarter)) |> pull(Quarter)
  value0 <- all_data |> filter(BankName == Bank, Quarter == (min_qtr - 1)) |> pull(UBPRE524.Value)
  
  event_data |> 
    mutate(observed = UBPRE524.Value, predicted = value0 + cumsum(prediction))|> 
      add_column(Partner = Partner)
}

result <- map2(partner_banks$Bank, partner_banks$Partner,map_prediction_to_partner, result_obs, data) |>
            list_rbind()


```


```{r}
#| warning: false
#| label: tbl-abnormal-xgboost
#| tbl-cap: "Credit Card Plans-30-89 DAYS P/D %: Abnormal Returns (original scale)"
#| tbl-subcap: 
#| - "Costco (Old) - AMERICAN EXPRESS NATIONAL BANK (1394676)"
#| - "Costco (New) - CITIBANK, N.A. (476810)"
#| - "Walmart (Old) - SYNCHRONY BANK (1216022)"
#| - "Walmart (New) - CAPITAL ONE, NATIONAL ASSOCIATION (112837)"
#| - "GAP (Old) - SYNCHRONY BANK (1216022)"
#| - "GAP (New) - BARCLAYS BANK DELAWARE (2980209)"
print_ar(result, "Costco", "AMERICAN EXPRESS NATIONAL BANK (1394676)")
print_ar(result, "Costco", "CITIBANK, N.A. (476810)")
print_ar(result, "Walmart", "SYNCHRONY BANK (1216022)")
print_ar(result, "Walmart", "CAPITAL ONE, NATIONAL ASSOCIATION (112837)")
print_ar(result, "GAP", "SYNCHRONY BANK (1216022)")
print_ar(result, "GAP", "BARCLAYS BANK DELAWARE (2980209)")
```

```{r}
#| eval: false
#| warning: false
#| echo: false

# Examine feature importance
importance_matrix <- xgb.importance(model = m)
print(importance_matrix)
xgb.plot.importance(importance_matrix = importance_matrix)
importance_matrix |> write_csv("data/results/xgbTree_importance_matrix.csv")

save_sv_dependence <- function(shp, feature, colour_var) {
  tryCatch({
  # Start PNG device
  png(filename = glue::glue("data/results/sv_dependence/{feature}_plot.png"), width = 800, height = 600)
  plot_obj <- sv_dependence(shp, feature, color_var)
  print(plot_obj)
  
  # Turn off the device
  dev.off()
  }, error = function(e) {
    # Handle errors
    dev.off()
    print(glue::glue("sv_dependence could not be plotted for {feature}"))
  })
}

```
```{r}
#| warning: false
#| eval: false
#| 
#The chart helps us understand which features are most important for a particular prediction and how they interact to produce the final outcome
library(shapviz)
shp <- shapviz(m,X_pred = dEst,X=data.matrix(estimation.predictors))
png(glue("images/estimation_postgroup_importance.png"), width = 800, height = 600)
shapviz::sv_importance(shp, kind = "beeswarm", show_numbers = TRUE)
dev.off()

png(glue("images/estimation_postgroup_waterfall.png"), width = 800, height = 600)
sv_waterfall(shp)
dev.off()

shp <- shapviz(m,X_pred = dObs,X=data.matrix(observation.predictors))
png(glue("images/obs_postgroup_importance.png"), width = 800, height = 600)
shapviz::sv_importance(shp, kind = "beeswarm", show_numbers = TRUE)
dev.off()
png(glue("images/obs_postgroup_waterfall.png"), width = 800, height = 600)
sv_waterfall(shp)
dev.off()

```

![Caption](images/estimation_postgroup_waterfall.png){#fig-shap-fall-post}
![Post](images/estimation_pregroup_waterfall.png){#fig-shap-fall-pre}