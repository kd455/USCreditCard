---
title: "XGBoost Model"
format: html
---

## Data Preparation {#sec-xgboost-data}
```{r}
#| warning: false
source("functions.R")
library(data.table)
library(xgboost)
library(Matrix)
library(caret)
library(patchwork)
library(shapviz)
set.seed(42)

target_label <- credit_card.target_label()

run_xgb.cv <- function(dtrain) {
  param_grid <- expand.grid(
    eta = c(0.01, 0.1, 0.3),
    max_depth = c(3, 6, 9),
    gamma = c(0,0.1,0.2),
    colsample_bytree = c(0.5, 0.8),
    min_child_weight = c(1, 5),
    subsample = c(0.5, 0.75),
    alpha = c(0, 0.1, 1, 10),
    lambda = c(0, 0.1, 1, 10)
  )

  # Placeholder for cross-validation results
  cv_results <- list()

  for(i in seq_len(nrow(param_grid))) {
    params <- list(
      booster = "gbtree",
      eta = param_grid$eta[i],
      max_depth = param_grid$max_depth[i],
      subsample = param_grid$subsample[i],
      colsample_bytree = param_grid$colsample_bytree[i],
      min_child_weight = param_grid$min_child_weight[i],
      alpha = param_grid$alpha[i],
      lambda = param_grid$lambda[i],
      objective = "reg:squarederror"
    )
    
    cv <- xgb.cv(params = params, data = dtrain, nrounds = 100, nfold = 5, early_stopping_rounds = 10)
    cv_results[[i]] <- list(params = params, cv_metrics = cv)
  }

  best_score <- Inf
  best_model <- NULL

  for(i in seq_along(cv_results)) {
    score <- min(cv_results[[i]]$cv_metrics$evaluation_log)
    if(score < best_score) {
      best_score <- score
      best_model <- cv_results[[i]]
    }
  }
  best_model$cv_metrics[4]$evaluation_log |> readr::write_csv("data/results/xgboost_cv_metrics.csv")
  best_model$params |> as.data.frame() |> readr::write_csv("data/results/xgboost_cv_params.csv")
}

run_xgb.caret.cv <- function(train.predictors, train.outcome, n_folds, n_repeats, model_name, method = "xgbTree", seeds) {
  
  if (method == "xgbTree") {
    grid <- expand.grid(eta = c(0.01, 0.1, 0.3),
                        max_depth = c(3, 6, 9),
                        gamma = c(0, 0.1),
                        colsample_bytree = c(0.5, 0.8),
                        min_child_weight = c(1, 5),
                        subsample = c(0.5, 0.75),
                        nrounds = c(100,150,200)
                      )
  } else {
    grid <- expand.grid(eta = 0.01,
                        alpha = c(0.1, 0.5, 1),
                        lambda = c(0.1, 1, 5, 10, 15, 25, 30, 50),
                        nrounds = 100
    )
  }

  control <- trainControl(method = "repeatedcv", number = n_folds, repeats = n_repeats, seeds = seeds)

  # Train the model
  model <- train(train.predictors, train.outcome,
                method = method,
                trControl = control,
                tuneGrid = grid,
                metric = "RMSE")

  # Save the plot
  png(glue("data/results/{method}_tuning_plot_{model_name}.png"), width = 800, height = 600)
  plot(model)
  dev.off() 

  #save tuning results
  model$results  |> readr::write_csv(glue("data/results/{method}_tuning_results_{model_name}.csv"))
  model$bestTune |> readr::write_csv(glue("data/results/{method}_best_tune_{model_name}.csv"))
  xgb.save(model = model$finalModel, fname = glue("data/results/best_{method}_{model_name}.model"))
}

# Define the number of folds and repeats
get_cv_params <- function() {
  n_folds <- 5
  n_repeats <- 3
  seeds <- lapply(1:(n_folds * n_repeats), function(x) sample.int(1000, 144))
  seeds[[length(seeds) + 1]] <- sample.int(1000, 1)  # The last seed for the final model

  list(n_folds = n_folds, n_repeats = n_repeats, seeds = seeds)
}

bank_fit_result <- function(bankname, data, train_test = "Train", target_name = "UBPRE524.diff") {
  tryCatch({
    X_filtered <- data |> filter(BankName == bankname) |> pull(predictions)
    Y_filtered <- data |> filter(BankName == bankname) |> pull(target_name)
    caret::postResample(pred = X_filtered, obs = Y_filtered) |> t() |>as_tibble() |>
      add_column(BankName = bankname,
                 .type = train_test) |> relocate(BankName,.type)
  }, error = function(e) {
    print(glue::glue("metrics error for {bankname}: {e}"))
  })
}

run_validation <- function(model, params, model_name) {
  m <- xgboost(booster = "gbtree", data = model$dtrain, params= params, nrounds=params$nrounds)

  bank_types <-  model$train |> distinct(BankName, BankType)

  #Training fit
  train_predictions <- predict(m, model$dtrain)
  # See Fit on unseen data: Generate Predictions
  test_predictions <- predict(m, model$dtest)

  train_result <- model$train |> mutate(predictions = train_predictions)
  test_result <- model$test |> mutate(predictions = test_predictions)

  results_est <- unique(model$train$BankName) |> 
                      map(bank_fit_result,train_result, "Train") |> list_rbind()

  results_test <- unique(model$train$BankName) |> 
                      map(bank_fit_result,test_result, "Test") |> list_rbind()
                        
  comb <- bind_rows(results_est, results_test) |> left_join(bank_types, by = join_by(BankName)) |>
            dplyr::select(BankName, BankType, .type, RMSE, MAE, Rsquared)

  comb |> write_csv(glue("data/results/xgbTree_validation_{model_name}.csv"))  
  
  importance_matrix  <- xgb.importance(names(model$dtrain), model = m)
  importance_matrix |> write_csv(glue("data/results/xgbTree_importmatrix_{model_name}.csv")) 

  #save images
  shap_train <- shapviz(m,X_pred = model$dtrain,X=data.matrix(model$train.predictors))
  shap_test <- shapviz(m,X_pred = model$dtest,X=data.matrix(model$test.predictors))

  png(glue("images/xgb_train_imp_{model_name}.png"), width = 800, height = 600)
  sv_importance(shap_train, kind = "beeswarm", show_numbers = TRUE)
  dev.off()

  png(glue("images/xgb_test_imp_{model_name}.png"), width = 800, height = 600)
  sv_importance(shap_test, kind = "beeswarm", show_numbers = TRUE)
  dev.off()
  
  png(glue("images/xgb_train_water_{model_name}.png"), width = 800, height = 600)
  sv_waterfall(shap_train)
  dev.off()

  png(glue("images/xgb_test_water_{model_name}.png"), width = 800, height = 600)
  sv_waterfall(shap_test)
  dev.off()
}
  
map_prediction_to_partner <- function(Bank, Partner, obs_data_w_qtr, all_data) {
  event_data <- obs_data_w_qtr |> 
                  filter(BankName == Bank) |> 
                    filter(!!as.name(Partner) >=0) |> head(8) 

  min_qtr <- event_data |> filter(Quarter == min(Quarter)) |> pull(Quarter)
  value0 <- all_data |> filter(BankName == Bank, Quarter == (min_qtr - 1)) |> pull(UBPRE524.Value)
  
  event_data |> 
    mutate(observed = UBPRE524.Value, predicted = value0 + cumsum(prediction))|> 
      add_column(Partner = Partner)
}

```

In our previous models, we utilised a panel dataset that captured temporal dynamics. However, machine learning algorithms require a dataset structured to reflect a single timeframe. Consequently, we need to transform our panel data into a cross-sectional format.

This entailed:

* Removing time-related factor 'Quarter' (for example, '2020 Q1').

* Excluding lagged values of the `r target_label` to prevent look-ahead bias. This ensures that our model does not inadvertently learn from future information which could result in overfitting.

* Excluding variables that are close proxies for `r target_label`, as listed in @tbl-xgboost-bias. These variables are closely tied to the target variable as they reflect ongoing delinquency beyond a 30-day period. This means that the model can indirectly learn future information which could result in overfitting.

```{r}
#| warning: false
#| label: tbl-xgboost-bias
#| tbl-cap: "Removed 'proxy' features"
get_ubpr_labels() |>
filter(Code %in% c("UBPRE411", "UBPRE523", "UBPRE521")) |> 
  dplyr::select(Code, Description = Desc) |>
    knitr::kable(format = "html")
```

```{r}
#| warning: false
model_data <- get_model_data()
all_data <- model_data$all_data |> as_tibble()           
partner_banks <- credit_card.partnerships.long()
estimation_data <- model_data$estimation_data
observation_data <- model_data$observation_data

log_cols_w_group <- all_data |>
                    dplyr::select(contains(".log.diff")) |> 
                    dplyr::select(-c(starts_with("UBPRE524.log.diff"))) |> names() 

log_cols_wo_group <- all_data |>
                    dplyr::select(contains(".log.diff")) |> 
                    dplyr::select(-c(starts_with("UBPRE524.log.diff"), 
                                     starts_with("UBPRE524.group"), 
                                     starts_with("UBPRE524.all")))  |> names() 


setup_train_test <- function(estimation_data, predictor_cols) {
  data <- estimation_data |> as_tibble() |>
            dplyr::select(c(UBPRE524.diff, BankName, BankType, all_of(predictor_cols), Qtr))  

  s <- createDataPartition(data$UBPRE524.diff, p = 0.70, list=FALSE)
  training <- data[s,]
  test <- data[-s,]

  # Convert the data to matrix and assign output variable
  train.outcome <- training$UBPRE524.diff
  train.predictors <- sparse.model.matrix(UBPRE524.diff ~ .-1, data = training)
  test.outcome <- test$UBPRE524.diff
  test.predictors <- sparse.model.matrix(UBPRE524.diff ~ .-1, data = test)

  # Convert the matrix objects to DMatrix objects
  dtrain <- xgb.DMatrix(train.predictors, label = train.outcome)
  dtest <- xgb.DMatrix(test.predictors, label = test.outcome)

  list(train = training, test = test, dtrain = dtrain, dtest = dtest, train.predictors = train.predictors, 
       test.predictors = test.predictors, train.outcome = train.outcome, test.outcome = test.outcome)
}

model_w_group <- setup_train_test(estimation_data, log_cols_w_group)
model_without_group <- setup_train_test(estimation_data, log_cols_wo_group)

```

```{r}
#| warning: false
#| echo: false
#| eval: false

cv_param <- get_cv_params()
run_xgb.caret.cv(model_w_group$train.predictors, model_w_group$train.outcome, cv_param$n_folds, cv_param$n_repeats, model_name= "model_w_group", method = "xgbTree", cv_param$seeds)
run_xgb.caret.cv(model_without_group$train.predictors, model_without_group$train.outcome, cv_param$n_folds, cv_param$n_repeats, model_name="model_without_group", method = "xgbTree", cv_param$seeds)

```

3 datasets were tested:

* *All data* including aggregated `r target_label` features.
* *Without Group* - All Data with group `r target_label` features removed.
* *Selected features*. To see if a simpler model would generalise better on unseen test data, a dataset of the top 10 important features from the *All data* model was selected (see @sec-xgboost-feature).

Furthermore, the seasonal factor *Qtr* (ranging from 1 to 4) was added to all datasets. Initial model testing revealed distinct seasonal trends in the residuals, as identified in the Autocorrelation Function (ACF) plots. Introducing *Qtr* attempts to help the model account for these seasonal patterns. An alternative approach could have involved de-seasonalising the data.

## Model Hyperparameter Tuning

In XGBoost we can tune the model hyperparameters e.g., maximum tree depth, to regularise the model so it does not overfit the training data.

We use Cross Validation to explore the hyperparameter space. This process helps identify the combination of parameters that best balance between overfitting (by avoiding too high variance) and underfitting (by avoiding too high bias).

We used 5-Fold Cross-Validation, repeated 3 times - each time with a different random division of the dataset into 5 folds. This means cross validation was performed a total of 15 train-test splits (3 repeats × 5 folds) to get an overall performance estimate.

We can see from @fig-xgcv-group that changes to the ETA hyperparameter, in particular, impacted the RMSE metric. ETA (value between 0 and 1) determines the weight of each tree's contribution to the final model. A smaller ETA means making smaller updates so the model is less likely to make large adjustments based on any single observation or pattern in the data i.e. it reduces the impact of outliers or noise. An eta of 0.1 for all datasets (see @tbl-xgboost-param) was selected. 

::: {#panel-xgtuning .panel-tabset}

### All Data
![Parameter Tuning for model with group features](images/xgbTree_tuning_plot_model_w_group.png){#fig-xgcv-group}

### Without Group
![Parameter Tuning for model without group features](images/xgbTree_tuning_plot_model_w_group.png){#fig-xgcv-wo-group}

### Selected Features
![Parameter Tuning for model with selected features](images/xgbTree_tuning_plot_model_select.png){#fig-xgcv-selected}

:::

```{r}
#| warning: false
#| label: tbl-xgboost-param
#| tbl-cap: Optimal tuning model parameters for each dataset
params_w_group <- read_csv("data/results/xgbTree_best_tune_model_w_group.csv",show_col_types = FALSE) |> 
                    add_column(model = "With Group")|> as.list()

params_wo_group <- read_csv("data/results/xgbTree_best_tune_model_without_group.csv",show_col_types = FALSE) |>
                    add_column(model = "Without Group") |> as.list()

params_select_group <- read_csv("data/results/xgbTree_best_tune_model_select.csv",show_col_types = FALSE) |>
                    add_column(model = "Selected") |> as.list()

bind_rows(params_w_group, params_wo_group,params_select_group) |> 
  pivot_longer(cols=c(-model)) |>
  pivot_wider(names_from=c(model)) |> rmarkdown::paged_table()  
```

## Train/Test Split Validation

```{r}
#| warning: false
#| results: hide
#| eval: false
run_validation(model_w_group, params_w_group,"w_group")
run_validation(model_without_group, params_wo_group,"wo_group")
```

The results in @tbl-xgtest-group show that the *Without Group* dataset has the largest discrepancy for RMSE and MAE between the Train and Test dataset. Similarly, the *All Data* dataset displays some overfitting in the results for all banks (@tbl-xgtest-all-3). The *Selected Feature* dataset across all banks (@tbl-xgtest-select-3) performed well and had the least discrepancy in our metrics.


::: {#panel-xgboost-test .panel-tabset}

### All Data
```{r}
#| warning: false
#| label: tbl-xgtest-all
#| tbl-cap: Comparing Accuracy Metrics for Training and Test data
#| tbl-subcap:
#| - "Accuracy Metrics on Training and Test data for sub-set of banks"
#| - "Mean Accuracy Metrics for our sub-set of banks"
#| - "Mean Accuracy Metrics across all banks"

banks = unique(observation_data$BankName)
val_results <- get_summary_validation.xgboost("w_group", banks)
val_results$tbl1 |> rmarkdown::paged_table()
val_results$tbl2 |> rmarkdown::paged_table()
val_results$tbl3 |> rmarkdown::paged_table()
```

### Without Group
```{r}
#| warning: false
#| label: tbl-xgtest-group
#| tbl-cap: Comparing Accuracy Metrics for Training and Test data#| 
#| tbl-subcap:
#| - "Accuracy Metrics on Training and Test data for sub-set of banks"
#| - "Mean Accuracy Metrics for our sub-set of banks"
#| - "Mean Accuracy Metrics across all banks"
val_wo_results <- get_summary_validation.xgboost("wo_group", banks)
val_wo_results$tbl1 |> rmarkdown::paged_table()
val_wo_results$tbl2 |> rmarkdown::paged_table()
val_wo_results$tbl3 |> rmarkdown::paged_table()
```
### Selected Features

```{r}
#| warning: false
#| label: tbl-xgtest-select
#| tbl-cap: Comparing Accuracy Metrics for Training and Test data
#| tbl-subcap:
#| - "Accuracy Metrics on Training and Test data for sub-set of banks"
#| - "Mean Accuracy Metrics for our sub-set of banks"
#| - "Mean Accuracy Metrics across all banks"
val_results <- get_summary_validation.xgboost("model_select", banks)
val_results$tbl1 |> rmarkdown::paged_table()
val_results$tbl2 |> rmarkdown::paged_table()
val_results$tbl3 |> rmarkdown::paged_table()
```

:::

## Feature Selection {#sec-xgboost-feature}

The plots below show the Gain (the improvement in accuracy the model gains from including a feature) and Frequency (how often each feature is split on across all boosting tree) of each feature relative to the first feature.

Notably the group aggregation of `r target_label` is the most important feature. Without it, we can see from  @fig-gain-wo that the model uses Household Debt Ratio (TDSP). Generally, portfolio features like *Unused Commitments on Credit Cards* were more important than exogenous economic measures.

::: {#fig-xgboost-importance .panel-tabset}

### All Data - Gain

```{r}
#| fig-height: 8
#| label: fig-gain-all
#| fig-cap: Feature importance using Gain 
importance_matrix_grp <- read_csv("data/results/xgbTree_importmatrix_w_group.csv",show_col_types = FALSE)

feature_desc <- get_feature_labels()

importance_matrix_lab <- importance_matrix_grp |> 
                          mutate(Code = stringr::str_split_i(Feature,"\\.",1)) |> 
                            left_join(feature_desc,by = join_by(Code)) |> 
                              mutate(Feature = paste(Feature, coalesce(Desc,""), sep = "\n")) |> data.table()

xgb.plot.importance(importance_matrix_lab, measure = "Gain", xlab = "Relative importance",top_n = 15,left_margin = 30,rel_to_first = TRUE)
```

### All Data - Frequency

```{r}
#| warning: false
#| fig-height: 8
#| label: fig-freq-all
#| fig-cap: Feature importance using Frequency 
xgb.ggplot.importance(importance_matrix_lab, measure = "Frequency", rel_to_first = TRUE,top_n = 15,left_margin = 30) + ylab("Freqency") + theme(legend.position = "none")

```

### Without Group - Gain

```{r}
#| fig-height: 8
#| label: fig-gain-wo
#| fig-cap: Feature importance using Gain 
importance_matrix_wo <- read_csv("data/results/xgbTree_importmatrix_wo_group.csv",show_col_types = FALSE)

feature_desc <- get_feature_labels()

importance_matrix_lab <- importance_matrix_wo |> 
                          mutate(Code = stringr::str_split_i(Feature,"\\.",1)) |> 
                            left_join(feature_desc,by = join_by(Code)) |> 
                              mutate(Feature = paste(Feature, coalesce(Desc,""), sep = "\n")) |> data.table()

xgb.plot.importance(importance_matrix_lab, measure = "Gain", xlab = "Relative importance", top_n = 15,left_margin = 30, rel_to_first = TRUE)
```

### Without Group - Frequency

```{r}
#| warning: false
#| label: fig-freq-wo
#| fig-cap: Feature importance using 
#| fig-height: 8
xgb.ggplot.importance(importance_matrix_lab, measure = "Frequency", rel_to_first = TRUE,top_n = 15,left_margin = 30) + ylab("Freqency") + theme(legend.position = "none")

```

:::

```{r}
#| warning: false
selected_features <- importance_matrix_grp |> 
                      dplyr::select(Feature) |> 
                        filter(!grepl("BankName", Feature)) |> 
                         top_n(10) |> add_row(Feature="Qtr") |> as.list()

cv_param <- get_cv_params()

model_select <- setup_train_test(estimation_data, selected_features$Feature)
```
```{r}
#| warning: false
#| eval: false
run_xgb.caret.cv(model_select$train.predictors, model_select$train.outcome, cv_param$n_folds, cv_param$n_repeats, model_name= "model_select", method = "xgbTree", cv_param$seeds)

params_select <- read_csv("data/results/xgbTree_best_tune_model_select.csv", show_col_types = FALSE) |> as.list()

run_validation(model_select, params_select,"model_select")
```
## Estimated Fit
```{r}
#| warning: false
#| results: hide
#| fig-keep: all
#| label: fig-plot-fit-xgboost
#| fig-cap: "Fitted vs. Observed for estimation data on sub-set of firms for model using *Selected Features*"
#| fig-height: 10
params_select <- read_csv("data/results/xgbTree_best_tune_model_select.csv", show_col_types = FALSE) |> as.list()

predictor_cols <- names(model_select$train)
est_data <- estimation_data |> as_tibble() |> dplyr::select(c(all_of(predictor_cols)))

# Convert the data to matrix and assign output variable
estimate.outcome <- est_data$UBPRE524.diff
estimate.predictors <- sparse.model.matrix(UBPRE524.diff ~ .-1, data = est_data)

# Convert the matrix objects to DMatrix objects
dEst <- xgb.DMatrix(estimate.predictors, label = estimate.outcome)

m <- xgboost(booster = "gbtree", data = dEst, params= params_select, nrounds=params_select$nrounds)

#Estimation fit
est_predictions <- predict(m, dEst)

bank_results <- estimation_data |> mutate(.fitted = est_predictions,
                                  .resid = UBPRE524.diff - est_predictions) |>
                dplyr::select(Quarter, BankName, BankType,UBPRE524.diff,.fitted,.resid) 
                  
bank_results |> 
  filter(BankName %in% observation_data$BankName) |> dplyr::select(-.resid) |>
    pivot_longer(cols = where(is.numeric))|> 
      ggplot(aes(x=Quarter, y = value, color=name)) + geom_line() + 
      facet_wrap(~BankType+BankName, ncol = 1) + 
      scale_colour_manual(values = c(".fitted" = "#D55E00", 
                                      UBPRE524.diff = "darkgrey")) + 
      theme(legend.position = "top", legend.title = element_blank(), legend.direction = "vertical")

```


## Shapley

Shapley additive explanations (SHAP) provide insights into how each feature influences the model's predictions. It's commonly used to interpret  predictions rather than regularisation [@Holzinger2022]. SHAP values can be averaged to assess overall feature importance across a dataset, while waterfall plots detail the feature contributions. Generally you would look at SHAP values for specific predictions, here we have averaged SHAP values for the training and test datasets.

::: {#fig-xgboost-shap .panel-tabset}

## Feature Importance - Test 

![Test data with group](images/xgb_test_imp_w_group.png)

![Test data without group](images/xgb_test_imp_wo_group.png)

![Test data selected features](images/xgb_test_imp_model_select.png)

## Feature Importance - Training

![Training data with group](images/xgb_train_imp_w_group.png)

![Training data without group](images/xgb_train_imp_wo_group.png)

![Training data selected features](images/xgb_train_imp_model_select.png)

## Waterfall - Test

![Test data without group](images/xgb_test_water_wo_group.png)

![Test data with group](images/xgb_test_water_w_group.png)

![Test data selected features](images/xgb_test_water_model_select.png)

## Waterfall - Training

![Training data with group](images/xgb_train_water_w_group.png)

![Training data selected features](images/xgb_train_water_model_select.png)

![Training data without group](images/xgb_train_water_wo_group.png)

:::


### Residual diagnostics {#sec-residual-check-xgboost}

XGBoost, as with other machine learning models, do not make assumptions about the distribution of residuals. But it is still worth inspecting the residuals to assess whether the model is adequately capturing the underlying patterns in the data. As discussed in @sec-xgboost-data the feature *Qtr* was added as the ACF plots were showing distinct seasonal patterns. Though improved, the ACF still show seasonality for some banks.

::: {#panel-xgboost-checks .panel-tabset}

### CITIBANK
```{r}
#| warning: false
#| label: fig-res-citi-xgboost
#| fig-cap: "XGBoost residuals"
bank <- "CITIBANK"
plot_bank_residual(bank, bank_results)

```

### SYNCHRONY
```{r}
#| warning: false
#| label: fig-res-sync-xgboost
#| fig-cap: "Best Bank Model residuals"
bank <- "SYNCHRONY"
plot_bank_residual(bank, bank_results)
```

### BARCLAYS
```{r}
#| warning: false
#| label: fig-res-bar-xgboost
#| fig-cap: "Model residuals"
bank <- "BARCLAYS"
plot_bank_residual(bank, bank_results)
```

### AMERICAN EXPRESS
```{r}
#| warning: false
#| label: fig-res-ae-xgboost
#| fig-cap: "Model residuals"
bank <- "AMERICAN EXPRESS"
plot_bank_residual(bank, bank_results)
```

### CAPITAL ONE

```{r}
#| warning: false
#| label: fig-res-cap-xgboost
#| fig-cap: "Model residuals"
bank <-  "CAPITAL ONE"
plot_bank_residual(bank, bank_results)
```

:::

## Prediction 

```{r}
#| warning: false
#| results: hide
#| fig-keep: all
#| label: fig-pred-observed-xgboost
#| fig-cap: "Prediction vs. Observed Event"
observation.outcome <- observation_data$UBPRE524.diff
observation.predictors <- sparse.model.matrix(UBPRE524.diff ~ .-1, data = observation_data  |> as_tibble() |> dplyr::select(all_of(predictor_cols)))

dObs <- xgb.DMatrix(observation.predictors, label = observation.outcome)
predictions <- predict(m, dObs)

result_obs <- observation_data |> mutate(prediction = predictions)

result <- map2(partner_banks$Bank, partner_banks$Partner,map_prediction_to_partner, result_obs, all_data) |> list_rbind()

map2(partner_banks$Bank, partner_banks$Partner, plot_prediction, result, all_data) 
```
## Abnormal Results
```{r}
#| warning: false
#| label: tbl-abnormal-xgboost
#| tbl-cap: "Credit Card Plans-30-89 DAYS P/D %: Abnormal Returns (original scale)"
#| tbl-subcap: 
#| - "Costco (Old) - AMERICAN EXPRESS NATIONAL BANK (1394676)"
#| - "Costco (New) - CITIBANK, N.A. (476810)"
#| - "Walmart (Old) - SYNCHRONY BANK (1216022)"
#| - "Walmart (New) - CAPITAL ONE, NATIONAL ASSOCIATION (112837)"
#| - "GAP (Old) - SYNCHRONY BANK (1216022)"
#| - "GAP (New) - BARCLAYS BANK DELAWARE (2980209)"
print_ar(result, "Costco", "AMERICAN EXPRESS NATIONAL BANK (1394676)")
print_ar(result, "Costco", "CITIBANK, N.A. (476810)")
print_ar(result, "Walmart", "SYNCHRONY BANK (1216022)")
print_ar(result, "Walmart", "CAPITAL ONE, NATIONAL ASSOCIATION (112837)")
print_ar(result, "GAP", "SYNCHRONY BANK (1216022)")
print_ar(result, "GAP", "BARCLAYS BANK DELAWARE (2980209)")
```

