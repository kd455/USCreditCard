---
title: "XGBoost Model"
format: html
---

Look-Ahead Bias: If data from after the test period is included in the training set, the model may inadvertently learn information from the future (known as "look-ahead bias"). This makes the model unrealistic and potentially over-optimistic in its predictive performance

```{r}
#| warning: false
source("functions.R")
library(data.table)
library(xgboost)
library(Matrix)
library(caret)
library(patchwork)
set.seed(42)

model_data <- get_model_data() |> 
                mutate(Qtr = as.factor(Qtr), BankType = as.factor(BankType), BankName = as.factor(BankName)) |> relocate(Qtr) |> as_tibble()

log_change_cols <- model_data |> select(contains(".log.diff")) |> 
                    select(-starts_with("UBPRE524.log.diff")) |> 
                      select(-c(starts_with("UBPRE524.group"), starts_with("UBPRE524.all"))) |>
                        names()                    

est_data_wQuarter <- model_data |> filter(is.na(Partnership)) |>
                              select(c(Quarter, UBPRE524.diff, BankName, BankType, log_change_cols, Qtr)) |>
                              drop_na()

estimation_data <- est_data_wQuarter |> select(-Quarter)

observation_data <- model_data |> filter(!is.na(Partnership)) |>
                      select(c(UBPRE524.diff, BankName, BankType, log_change_cols, Qtr)) |> 
                        drop_na()

#need to have consistent factors
observation_data$BankName <- factor(observation_data$BankName , levels = levels(estimation_data$BankName))

s <- createDataPartition(estimation_data$UBPRE524.diff, p = 0.70, list=FALSE)
training <- estimation_data[s,]
test <- estimation_data[-s,]

# Convert the data to matrix and assign output variable
train.outcome <- training$UBPRE524.diff
train.predictors <- sparse.model.matrix(UBPRE524.diff ~ .-1, data = training)
test.outcome <- test$UBPRE524.diff
test.predictors <- sparse.model.matrix(UBPRE524.diff ~ .-1, data = test)

# Convert the matrix objects to DMatrix objects
dtrain <- xgb.DMatrix(train.predictors, label = train.outcome)
dtest <- xgb.DMatrix(test.predictors, label = test.outcome)

```

```{r}
#| warning: false
#| echo: false
#| eval: false
run_xgb.cv <- function(dtrain) {
  param_grid <- expand.grid(
    eta = c(0.01, 0.1, 0.3),
    max_depth = c(3, 6, 9),
    gamma = c(0,0.1,0.2),
    colsample_bytree = c(0.5, 0.8),
    min_child_weight = c(1, 5),
    subsample = c(0.5, 0.75),
    alpha = c(0, 0.1, 1, 10),
    lambda = c(0, 0.1, 1, 10)
  )

  # Placeholder for cross-validation results
  cv_results <- list()

  for(i in seq_len(nrow(param_grid))) {
    params <- list(
      booster = "gbtree",
      eta = param_grid$eta[i],
      max_depth = param_grid$max_depth[i],
      subsample = param_grid$subsample[i],
      colsample_bytree = param_grid$colsample_bytree[i],
      min_child_weight = param_grid$min_child_weight[i],
      alpha = param_grid$alpha[i],
      lambda = param_grid$lambda[i],
      objective = "reg:squarederror"
    )
    
    cv <- xgb.cv(params = params, data = dtrain, nrounds = 100, nfold = 5, early_stopping_rounds = 10)
    cv_results[[i]] <- list(params = params, cv_metrics = cv)
  }

  best_score <- Inf
  best_model <- NULL

  for(i in seq_along(cv_results)) {
    score <- min(cv_results[[i]]$cv_metrics$evaluation_log)
    if(score < best_score) {
      best_score <- score
      best_model <- cv_results[[i]]
    }
  }
  best_model$cv_metrics[4]$evaluation_log |> readr::write_csv("data/results/xgboost_cv_metrics.csv")
  best_model$params |> as.data.frame() |> readr::write_csv("data/results/xgboost_cv_params.csv")
}

run_xgb.caret.cv <- function(train.predictors, train.outcome, n_folds, n_repeats, method = "xgbTree", seeds) {
  
  if (method == "xgbTree") {
    grid <- expand.grid(eta = c(0.01, 0.1, 0.3),
                        max_depth = c(3, 6, 9),
                        gamma = c(0, 0.1),
                        colsample_bytree = c(0.5, 0.8),
                        min_child_weight = c(1, 5),
                        subsample = c(0.5, 0.75),
                        nrounds = c(100,150,200)
                      )
  } else {
    grid <- expand.grid(eta = 0.01,
                        alpha = c(0.1, 0.5, 1),
                        lambda = c(0.1, 1, 5, 10, 15, 25, 30, 50),
                        nrounds = 100
    )
  }

  set.seed(123)
  control <- trainControl(method = "repeatedcv", number = n_folds, repeats = n_repeats, seeds = seeds)

  # Train the model
  model <- train(train.predictors, train.outcome,
                method = method,
                trControl = control,
                tuneGrid = grid,
                metric = "RMSE")

  # Save the plot
  png(glue("data/results/{method}_tuning_plot.png"), width = 800, height = 600)
  plot(model)
  dev.off() 

  #save tuning results
  model$results  |> readr::write_csv(glue("data/results/{method}_tuning_results.csv"))
  model$bestTune |> readr::write_csv(glue("data/results/{method}_best_tune.csv"))
  xgb.save(model = model$finalModel, fname = glue("data/results/best_{method}_model.model"))
}

# Define the number of folds and repeats
n_folds <- 5
n_repeats <- 3
seeds <- lapply(1:(n_folds * n_repeats), function(x) sample.int(1000, 144))
seeds[[length(seeds) + 1]] <- sample.int(1000, 1)  # The last seed for the final model

run_xgb.caret.cv(train.predictors, train.outcome, n_folds, n_repeats, method = "xgbTree", seeds)

```
## Model Selection
5-Fold Cross-Validation:

The dataset is divided into 5 equal (or nearly equal) parts, or "folds".
For each fold:
The fold is temporarily held out as the test set.
The model is trained on the remaining 4 folds combined.
The trained model is then tested on the held-out fold to evaluate its performance.
This process is repeated 5 times, once for each fold, ensuring that every data point is used exactly once as part of the test set.
The 5 performance metrics (e.g., accuracy, MSE) obtained from each fold are typically averaged to get a single performance estimate for the model.
Repeating the Process 3 Times:

After completing the 5-fold cross-validation once, you repeat the entire process 2 more times, each time with a different random division of the dataset into 5 folds.
This means you will perform a total of 15 train-test splits (3 repeats Ã— 5 folds).
For each repeat, you calculate an average performance metric over its 5 folds, just like in the single round of 5-fold cross-validation.
Finally, you might average the performance metrics across all 3 repeats to get an overall performance estimate, or you might assess the variability of the model performance across repeats.

![Caption](images/xgbTree_tuning_plot.png){#fig-xgcv}

## Model Results

On estimation data

```{r}
#| warning: false
#| results: hide
params <- read_csv("data/results/xgbTree_best_tune.csv",show_col_types = FALSE) |> as.list()

#train model   
m <- xgboost(booster = "gbtree", data = dtrain, params= params, nrounds=params$nrounds)
#Training fit
train_predictions <- predict(m, dtrain)
# See Fit on unseen data: Generate Predictions
test_predictions <- predict(m, dtest)


```
```{r}
#| warning: false
#| label: tbl-xgboost-param
#| tbl-cap: Best model parameters 
params |> as_tibble() |> pivot_longer(cols = everything()) |> rmarkdown::paged_table()   
```

```{r}
#| warning: false
#| label: fig-plot-fit-xgboost
#| fig-cap: "Fitted vs. Observed for estimation data on sub-set of firms"
#| fig-height: 10
bank_data_pred <- est_data_wQuarter[s,]
bank_data_test <- est_data_wQuarter[-s,]
bank_data_pred$train_pred <- train_predictions
bank_data_test$test_pred <- test_predictions
bank_results <- bind_rows(bank_data_test, bank_data_pred) |> 
                mutate(resid = UBPRE524.diff - coalesce(test_pred,train_pred)) |>
                select(Quarter, BankName, BankType,UBPRE524.diff,train_pred,test_pred, resid) 
                  
bank_results |> 
  filter(BankName %in% observation_data$BankName) |> select(-resid) |>
    pivot_longer(cols = where(is.numeric))|> 
      ggplot(aes(x=Quarter, y = value, color=name)) + geom_line() + geom_point()+
      facet_wrap(~BankType+BankName, ncol = 1) + 
      scale_colour_manual(values = c("train_pred" = "#1B9E77", 
                                     "test_pred" = "#D55E00", 
                                      UBPRE524.diff = "darkgrey")) + 
      theme(legend.position = "top", legend.title = element_blank(), legend.direction = "vertical")

```
### Residual diagnostics {#sec-residual-check-xgboost}

::: {#fig-xgboost-checks .panel-tabset}

```{r}
#| warning: false
plot_xgboost_residual <- function(fuzzy_bankname, data) {
  bank_data <- data |> filter(grepl(fuzzy_bankname,BankName))

  plot1 <- bank_data |> ggplot(aes(x = Quarter, y = resid)) + geom_line() + geom_point() 
  plot2 <- acf(bank_data$resid,plot = FALSE) %>% forecast::autoplot() + labs(title = element_blank())
  plot3 <- bank_data |> ggplot(aes(x = resid)) + geom_histogram(bins = 10) 
  plot_layout <- plot1 / 
               (plot2 | plot3) + 
               plot_layout(widths = c(7, 3)) # This sets the widths for plots 2 and 3
  plot_layout              
}

```
### CITIBANK
```{r}
#| warning: false
#| label: fig-res-citi-xgboost
#| fig-cap: "XGBoost Residuals"
bank <- "CITIBANK"
plot_xgboost_residual(bank, bank_results)

```

### SYNCHRONY
```{r}
#| warning: false
#| label: fig-res-sync-xgboost
#| fig-cap: "Best Bank Model Residuals"
bank <- "SYNCHRONY"
plot_xgboost_residual(bank, bank_results)
```

### BARCLAYS
```{r}
#| warning: false
#| label: fig-res-bar-xgboost
#| fig-cap: "Model Residuals"
bank <- "BARCLAYS"
plot_xgboost_residual(bank, bank_results)
```

### AMERICAN EXPRESS
```{r}
#| warning: false
#| label: fig-res-ae-xgboost
#| fig-cap: "Model Residuals"
bank <- "AMERICAN EXPRESS"
plot_xgboost_residual(bank, bank_results)
```

### CAPITAL ONE

```{r}
#| warning: false
#| label: fig-res-cap-xgboost
#| fig-cap: "Model Residuals"
bank <-  "CAPITAL ONE"
plot_xgboost_residual(bank, bank_results)
```

:::

### Residuals White Noise

The sub-set of banks analysed in @sec-residual-check-xgboost did not exhibit significant autocorrelation.

@tbl-lb-xgboost lists banks where we reject the null hypothesis of the Ljungâ€“Box test, indicating the presence of significant autocorrelation in the residuals. 

Banks not listed have residuals that are indistinguishable from a white noise series i.e. have uncorrelated observations and with constant variance.
```{r}
#| warning: false
#| label: tbl-lb-xgboost
#| tbl-cap: Ljungâ€“Box results where P-value < Significance Level of 0.05
bank_results |>
 as_tsibble(index = Quarter, key = BankName) |> features(resid, ljung_box) |> 
  mutate(across(where(is.numeric), \(x) round(x,4))) |> 
    filter(lb_pvalue <= 0.05)|> select(BankName,lb_stat,lb_pvalue) |>
      rmarkdown::paged_table()
```

## Train/Test Split Validation

```{r}
#| warning: false
#| label: tbl-xgboost-test
#| tbl-cap: Comparing Accuracy Metrics for Training and Test data
#| tbl-subcap: 
#| - Accuracy Metrics on Training and Test data for sub-set of banks
#| - Mean Accuracy Metrics for our sub-set of banks
#| - Mean Accuracy Metrics across all banks

bank_fit_result <- function(bankname, data, train_test = "Train", target_name = "UBPRE524.diff") {
  tryCatch({
    X_filtered <- data |> filter(BankName == bankname) |> pull(predictions)
    Y_filtered <- data |> filter(BankName == bankname) |> pull(target_name)
    caret::postResample(pred = X_filtered, obs = Y_filtered) |> t() |>as_tibble() |>
      add_column(BankName = bankname,
                 .type = train_test) |> relocate(BankName,.type)
  }, error = function(e) {
    print(glue::glue("metrics error for {bankname}: {e}"))
  })
}

train_result <- training |> mutate(predictions = train_predictions)
test_result <- test |> mutate(predictions = test_predictions)

results_est <- unique(estimation_data$BankName) |> 
                    map(bank_fit_result,train_result, "Train") |> list_rbind()

results_test <- unique(estimation_data$BankName) |> 
                    map(bank_fit_result,test_result, "Test") |> list_rbind()
                       

comb <- bind_rows(results_est, results_test) |>
          mutate(across(where(is.numeric), \(x) round(x,4))) |> 
            select(BankName, .type, RMSE, MAE, Rsquared)

comb |> filter(BankName %in% observation_data$BankName) |> arrange(BankName,.type) |> rmarkdown::paged_table()     

comb |> filter(BankName %in% observation_data$BankName) |> 
  group_by(.type) |> summarise(across(where(is.numeric), \(x) mean(x, na.rm = TRUE))) |>
    rmarkdown::paged_table()  

comb |> group_by(.type) |> 
  summarise(across(where(is.numeric), \(x) mean(x, na.rm = TRUE))) |> 
    rmarkdown::paged_table()   

```
## Prediction 

```{r}
#| warning: false
# Convert the data to matrix and assign output variable
estimation.outcome <- estimation_data$UBPRE524.diff
estimation.predictors <- sparse.model.matrix(UBPRE524.diff ~ .-1, data = estimation_data)
observation.outcome <- observation_data$UBPRE524.diff
observation.predictors <- sparse.model.matrix(UBPRE524.diff ~ .-1, data = observation_data)

# Convert the matrix objects to DMatrix objects
dEst <- xgb.DMatrix(estimation.predictors, label = estimation.outcome)
dObs <- xgb.DMatrix(observation.predictors, label = observation.outcome)

#train model   
m <- xgboost(booster = "gbtree", data = dEst, params= params, nrounds=params$nrounds)
#predit
predictions <- predict(m, dObs)


#xgb.dump(m, with_stats = TRUE)[0:10]
```


```{r}
#| eval: false
# Examine feature importance
#cols <- names(training[, -which(names(training) == "UBPRE524.diff")])
importance_matrix <- xgb.importance(model = m)
print(importance_matrix)
xgb.plot.importance(importance_matrix = importance_matrix)
importance_matrix |> write_csv("data/results/xgbTree_importance_matrix.csv")

library(shapviz)
save_sv_dependence <- function(shp, feature, colour_var) {
  tryCatch({
  # Start PNG device
  png(filename = glue::glue("data/results/sv_dependence/{feature}_plot.png"), width = 800, height = 600)
  plot_obj <- sv_dependence(shp, feature, color_var)
  print(plot_obj)
  
  # Turn off the device
  dev.off()
  }, error = function(e) {
    # Handle errors
    dev.off()
    print(glue::glue("sv_dependence could not be plotted for {feature}"))
  })
}


shp <- shapviz(m,X_pred = dtrain,X=data.matrix(train.predictors))
shapviz::sv_importance(shp, kind = "bar", show_numbers = TRUE)

names(estimation_data) |> walk(\(x) save_sv_dependence(shp, x, NULL))


sv_dependence(shp, "UBPRE524.group.log.diff", color_var = NULL)
sv_dependence(shp, "UBPRE263.log.diff.lag1", color_var = NULL)
sv_dependence(shp, "UBPRB538.log.diff", color_var = NULL)
sv_dependence(shp, "UBPR3815.log.diff", color_var = NULL)
sv_force(shp, row_id = 1)
sv_waterfall(shp, row_id = 1)
```